MNIST solved at 95% accuracy with a simple neural network built from scratch in Python & NumPy.


Gradients were computed following: https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf.

It lacks a proof for differentiating cross_entropy(softmax(x2), lables) with respect to x2.

The proof is well covered here: https://stats.stackexchange.com/questions/235528/backpropagation-with-softmax-cross-entropy.


The model itself is in /solving_MNIST/notebook.ipynb.
